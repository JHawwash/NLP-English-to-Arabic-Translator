{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aafBvcuhiTAE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding, Dot, Concatenate, SimpleRNN, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrWFY4vIiTAH",
        "outputId": "ef593a8d-834c-4407-a039-5733d425edec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "file = 'ara_eng.txt'\n",
        "\n",
        "data = open('ara_eng.txt', \"r\", encoding = \"UTF-8\")\n",
        "data = data.read().split('\\n')\n",
        "\n",
        "# Splitting the text into (English, Arabic) pairs\n",
        "for i in range(0, len(data)):\n",
        "  data[i] = data[i].split('\\t')\n",
        "  data[i] = list(map(str.lower, data[i]))\n",
        "\n",
        "data[-1]\n",
        "\n",
        "data.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rxtgumLuuMDN"
      },
      "outputs": [],
      "source": [
        "# Storing the English and Arabic sentences in different lists\n",
        "eng = [row[0] for row in data]\n",
        "ara = [row[1] for row in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zoQHpioiTAH",
        "outputId": "9f32e40b-9340-4f7f-9abd-13e8cc584bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 3631\n",
            "Arabic Vocabulary Size: 10520\n",
            "English Max Length: 11\n",
            "Arabic Max Length: 14\n"
          ]
        }
      ],
      "source": [
        "english_sentences = eng[:10000]\n",
        "arabic_sentences = ara[:10000]\n",
        "# Tokenizing the English sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "sequences_english = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Tokenizing the Arabic sentences\n",
        "arabic_tokenizer = Tokenizer()\n",
        "arabic_tokenizer.fit_on_texts(arabic_sentences)\n",
        "sequences_arabic = arabic_tokenizer.texts_to_sequences(arabic_sentences)\n",
        "\n",
        "\n",
        "# Finding the vocabulary size of English and Arabic\n",
        "vocab_size_english = len(english_tokenizer.word_index) + 1\n",
        "vocab_size_arabic = len(arabic_tokenizer.word_index) + 1\n",
        "\n",
        "# Finding the maximum length of English and Arabic sentences\n",
        "max_length_english = max(len(seq) for seq in sequences_english)\n",
        "max_length_arabic = max(len(seq) for seq in sequences_arabic)\n",
        "\n",
        "\n",
        "# Padding the English and Arabic sequences\n",
        "padded_english = pad_sequences(sequences_english, maxlen = max_length_english, padding = 'post')\n",
        "padded_arabic = pad_sequences(sequences_arabic, maxlen = max_length_arabic, padding = 'post')\n",
        "\n",
        "print(f\"English Vocabulary Size: {vocab_size_english}\")\n",
        "print(f\"Arabic Vocabulary Size: {vocab_size_arabic}\")\n",
        "print(f\"English Max Length: {max_length_english}\")\n",
        "print(f\"Arabic Max Length: {max_length_arabic}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyC5_QGiiTAI",
        "outputId": "36912dcc-9d60-4d29-d621-eaa25f9ccfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 11, 1)\n",
            "(10000, 14, 1)\n"
          ]
        }
      ],
      "source": [
        "# Reshaping the English and Arabic sequences \n",
        "padded_english = padded_english.reshape(*padded_english.shape, 1)\n",
        "padded_arabic = padded_arabic.reshape(*padded_arabic.shape, 1)\n",
        "\n",
        "print(padded_english.shape)\n",
        "print(padded_arabic.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSdX4y8LiTAI",
        "outputId": "70adabf1-961d-4624-bbe3-a04397a8a306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 11, 128)      464768      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 14, 128)      1346560     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 11, 64),     49408       ['embedding[0][0]']              \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 14, 64),     49408       ['embedding_1[0][0]',            \n",
            "                                 (None, 64),                      'lstm[0][1]',                   \n",
            "                                 (None, 64)]                      'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 14, 11)       0           ['lstm_1[0][0]',                 \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 14, 11)       0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dot_1 (Dot)                    (None, 14, 64)       0           ['activation[0][0]',             \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 14, 128)      0           ['dot_1[0][0]',                  \n",
            "                                                                  'lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 14, 10520)    1357080     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,267,224\n",
            "Trainable params: 3,267,224\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Defining the encoder-decoder model with attention\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape = (max_length_english,))\n",
        "encoder_embedding = Embedding(input_dim = vocab_size_english, output_dim = 128, input_length = max_length_english, mask_zero = True)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(64, return_sequences = True, return_state = True)(encoder_embedding)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape = (max_length_arabic,))\n",
        "decoder_embedding = Embedding(input_dim = vocab_size_arabic, output_dim = 128)(decoder_inputs)\n",
        "decoder_lstm = LSTM(64, return_sequences = True, return_state = True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state = [state_h, state_c])\n",
        "\n",
        "# Attention Layer\n",
        "attention = Dot(axes = [2, 2])([decoder_outputs, encoder_outputs])\n",
        "attention = Activation('softmax')(attention)\n",
        "context = Dot(axes = [2, 1])([attention, encoder_outputs])\n",
        "concat_input = Concatenate(axis = -1)([context, decoder_outputs])\n",
        "\n",
        "# Output Layer\n",
        "decoder_dense = Dense(vocab_size_arabic, activation='softmax')\n",
        "output = decoder_dense(concat_input)\n",
        "\n",
        "# Model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "# Compiling the model\n",
        "lr = 0.01\n",
        "model.compile(optimizer = Adam(learning_rate = lr), loss = sparse_categorical_crossentropy, metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioI1aqnXzlFM",
        "outputId": "91139cbb-12ab-4036-d73f-a9395ee43ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10 - Loss: 2.1159711678822837 - Accuracy: 0.7570756054841555 - Test Accuracy: 0.827150821685791\n",
            "Epoch: 2/10 - Loss: 1.13325069214289 - Accuracy: 0.849845466705469 - Test Accuracy: 0.8830793499946594\n",
            "Epoch: 3/10 - Loss: 0.6807916859785715 - Accuracy: 0.8993532520074111 - Test Accuracy: 0.9137460589408875\n",
            "Epoch: 4/10 - Loss: 0.3967553907288955 - Accuracy: 0.9336152116839702 - Test Accuracy: 0.9425714015960693\n",
            "Epoch: 5/10 - Loss: 0.21043648284215194 - Accuracy: 0.9629478582586998 - Test Accuracy: 0.9777539968490601\n",
            "Epoch: 6/10 - Loss: 0.07248771717795768 - Accuracy: 0.9884744152808801 - Test Accuracy: 0.9970317482948303\n",
            "Epoch: 7/10 - Loss: 0.019225011117016085 - Accuracy: 0.9970524318707294 - Test Accuracy: 0.9973571300506592\n",
            "Epoch: 8/10 - Loss: 0.01665861874472541 - Accuracy: 0.9971883640839503 - Test Accuracy: 0.9982619285583496\n",
            "Epoch: 9/10 - Loss: 0.00961310723654699 - Accuracy: 0.9983258980971116 - Test Accuracy: 0.9994285702705383\n",
            "Epoch: 10/10 - Loss: 0.003393353548185237 - Accuracy: 0.999427657860976 - Test Accuracy: 0.9998254179954529\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "batch_size = 32\n",
        "num_batches = len(padded_english) // batch_size\n",
        "test_size = 1000\n",
        "\n",
        "padded_english_test = padded_english[:-test_size]\n",
        "padded_arabic_test = padded_arabic[:-test_size]\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    \n",
        "    for batch in range(num_batches):\n",
        "        start = batch * batch_size\n",
        "        end = start + batch_size\n",
        "        english_batch = padded_english[start:end]\n",
        "        arabic_batch = padded_arabic[start:end]\n",
        "        \n",
        "        metrics = model.train_on_batch([english_batch, arabic_batch], arabic_batch)\n",
        "        loss = metrics[0]\n",
        "        accuracy = metrics[1]\n",
        "        \n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "    \n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "    \n",
        "    test_loss, test_accuracy = model.evaluate([padded_english_test, padded_arabic_test], padded_arabic_test, verbose=0)\n",
        "    \n",
        "    print(f\"Epoch: {epoch + 1}/{epochs} - Loss: {avg_loss} - Accuracy: {avg_accuracy} - Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "model.save('RNN_ENCODER_DECODER.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L2Ppu2IQqJWu"
      },
      "outputs": [],
      "source": [
        "model = load_model('RNN_ENCODER_DECODER.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LsMyH06tiTAJ"
      },
      "outputs": [],
      "source": [
        "def logits_to_sentence(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZl1BOqdiTAK",
        "outputId": "45b56549-e23b-4d68-9aeb-9d7ab735c87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Index: 2593\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "\n",
            "\n",
            "English Sentence: he looks suspicious.\n",
            "Actual Arabic Sentence: يبدو كأنه شخص مثير للشك.\n",
            "\n",
            "\n",
            "Predicted Arabic Sentence: يبدو كأنه شخص مثير للشك         \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "index = random.randint(0, len(english_sentences) - 1)\n",
        "print(\"Random Index:\", index)\n",
        "\n",
        "the_english_sentence = english_sentences[index]\n",
        "the_arabic_sentence = arabic_sentences[index]\n",
        "the_predicted_translation = logits_to_sentence(model.predict([padded_english[index:index + 1], padded_arabic[index:index + 1]])[0], arabic_tokenizer)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"English Sentence: {the_english_sentence}\")\n",
        "print(f\"Actual Arabic Sentence: {the_arabic_sentence}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predicted Arabic Sentence: {the_predicted_translation}\")\n",
        "print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}