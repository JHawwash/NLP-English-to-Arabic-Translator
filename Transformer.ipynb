{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "import numpy as np\n",
        "!pip install transformers\n",
        "from transformers import TFAutoModel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmurHZP_aSAj",
        "outputId": "af18db91-fdef-4d65-e9e1-91c37e9d43f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = 'ara_eng.txt'\n",
        "\n",
        "data = open('ara_eng.txt', \"r\", encoding = \"UTF-8\")\n",
        "data = data.read().split('\\n')\n",
        "\n",
        "# Splitting the text into (English, Arabic) pairs\n",
        "for i in range(0, len(data)):\n",
        "  data[i] = data[i].split('\\t')\n",
        "  data[i] = list(map(str.lower, data[i]))\n",
        "\n",
        "data[-1]\n",
        "\n",
        "data.pop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ELWYzpAcC04",
        "outputId": "0e9dc7ef-7991-4c3c-a3ba-9f44f13ad6b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the English and Arabic sentences in different lists\n",
        "eng = [row[0] for row in data]\n",
        "ara = [row[1] for row in data]"
      ],
      "metadata": {
        "id": "UPkmKcsCcE08"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences = eng[:10000]\n",
        "arabic_sentences = ara[:10000]\n",
        "# Tokenizing the English sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "sequences_english = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Tokenizing the Arabic sentences\n",
        "arabic_tokenizer = Tokenizer()\n",
        "arabic_tokenizer.fit_on_texts(arabic_sentences)\n",
        "sequences_arabic = arabic_tokenizer.texts_to_sequences(arabic_sentences)\n",
        "\n",
        "\n",
        "# Finding the vocabulary size of English and Arabic\n",
        "vocab_size_english = len(english_tokenizer.word_index) + 1\n",
        "vocab_size_arabic = len(arabic_tokenizer.word_index) + 1\n",
        "\n",
        "# Finding the maximum length of English and Arabic sentences\n",
        "max_length_english = max(len(seq) for seq in sequences_english)\n",
        "max_length_arabic = max(len(seq) for seq in sequences_arabic)\n",
        "\n",
        "\n",
        "# Padding the English and Arabic sequences\n",
        "padded_english = pad_sequences(sequences_english, maxlen = max_length_english, padding = 'post')\n",
        "padded_arabic = pad_sequences(sequences_arabic, maxlen = max_length_arabic, padding = 'post')\n",
        "\n",
        "print(f\"English Vocabulary Size: {vocab_size_english}\")\n",
        "print(f\"Arabic Vocabulary Size: {vocab_size_arabic}\")\n",
        "print(f\"English Max Length: {max_length_english}\")\n",
        "print(f\"Arabic Max Length: {max_length_arabic}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO0x57cNcG2e",
        "outputId": "c849433c-0ab9-4947-bc07-80c641d7b68e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 3631\n",
            "Arabic Vocabulary Size: 10520\n",
            "English Max Length: 11\n",
            "Arabic Max Length: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the English and Arabic sequences \n",
        "padded_english = padded_english.reshape(*padded_english.shape, 1)\n",
        "padded_arabic = padded_arabic.reshape(*padded_arabic.shape, 1)\n",
        "\n",
        "print(padded_english.shape)\n",
        "print(padded_arabic.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdNVyNAdcKDg",
        "outputId": "94ef65b6-10cb-4910-b3f8-6cb1cbc5549e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 11, 1)\n",
            "(10000, 14, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = Input(shape=(max_length_english,), dtype='int32')\n",
        "embedding = Embedding(input_dim=vocab_size_english, output_dim=128)(input_sequence)\n",
        "\n",
        "transformer_model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "encoder_outputs = transformer_model(input_sequence)[0]\n",
        "\n",
        "decoder_inputs = Input(shape=(max_length_arabic,), dtype='int32')\n",
        "decoder_embedding = Embedding(input_dim=vocab_size_arabic, output_dim=128)(decoder_inputs)\n",
        "\n",
        "encoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(embedding)\n",
        "\n",
        "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "decoder_dense = Dense(vocab_size_arabic, activation='softmax')\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "enc_dec_model = Model([input_sequence, decoder_inputs], output)\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "enc_dec_model.compile(optimizer=optimizer, loss=sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "enc_dec_model.summary()\n",
        "\n",
        "batch_size = 32\n",
        "num_batches = len(padded_english) // batch_size\n",
        "\n",
        "val_size = 1000\n",
        "eng_pad_val = padded_english[-val_size:]\n",
        "ara_pad_val = padded_arabic[-val_size:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2AQhzwYagMD",
        "outputId": "143babf8-3a3e-4c7f-f3fb-f08bfe35f48e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 11)]         0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 14)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 11, 128)      464768      ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 14, 128)      1346560     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 11, 64),     49408       ['embedding_2[0][0]']            \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 14, 64),     49408       ['embedding_3[0][0]',            \n",
            "                                 (None, 64),                      'lstm_2[0][1]',                 \n",
            "                                 (None, 64)]                      'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 14, 10520)    683800      ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,593,944\n",
            "Trainable params: 2,593,944\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    \n",
        "    for batch in range(num_batches):\n",
        "        indices = np.random.choice(len(padded_english), size=batch_size, replace=False)\n",
        "        eng_batch = padded_english[indices]\n",
        "        ara_batch = padded_arabic[indices]\n",
        "\n",
        "        loss, accuracy = enc_dec_model.train_on_batch([eng_batch, ara_batch], ara_batch)\n",
        "\n",
        "        total_loss += loss\n",
        "        total_accuracy += accuracy\n",
        "    \n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss, val_accuracy = enc_dec_model.evaluate([eng_pad_val, ara_pad_val], ara_pad_val, verbose=0)\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "\n",
        "    print(\"Epoch: {}/{} - Avg. Loss: {:.4f} - Avg. Accuracy: {:.4f} - Val Loss: {:.4f} - Val Accuracy: {:.4f}\".format(\n",
        "        epoch + 1, num_epochs, avg_loss, avg_accuracy, val_loss, val_accuracy))\n",
        "\n",
        "# Function to convert logits to a sentence\n",
        "def logits_to_sentence(logits, tokenizer):\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ''\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "# Example sentence translation\n",
        "index = 14\n",
        "print(\"The English sentence is: {}\".format(english_sentences[index]))\n",
        "print(\"The Arabic sentence is: {}\".format(arabic_sentences[index]))\n",
        "print('The predicted Arabic sentence is:')\n",
        "predicted_sentence = logits_to_sentence(\n",
        "    enc_dec_model.predict([padded_english[index:index + 1], padded_arabic[index:index + 1]])[0],\n",
        "    arabic_tokenizer)\n",
        "print(predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0HVCbiLbCi9",
        "outputId": "e9387b52-07ee-41c7-c92a-d85000a7e98d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15 - Avg. Loss: 3.3423 - Avg. Accuracy: 0.7079 - Val Loss: 3.3326 - Val Accuracy: 0.5663\n",
            "Epoch: 2/15 - Avg. Loss: 2.0997 - Avg. Accuracy: 0.7144 - Val Loss: 3.1153 - Val Accuracy: 0.5700\n",
            "Epoch: 3/15 - Avg. Loss: 1.9902 - Avg. Accuracy: 0.7244 - Val Loss: 2.9838 - Val Accuracy: 0.5866\n",
            "Epoch: 4/15 - Avg. Loss: 1.8931 - Avg. Accuracy: 0.7301 - Val Loss: 2.8498 - Val Accuracy: 0.5970\n",
            "Epoch: 5/15 - Avg. Loss: 1.7896 - Avg. Accuracy: 0.7411 - Val Loss: 2.6920 - Val Accuracy: 0.6157\n",
            "Epoch: 6/15 - Avg. Loss: 1.6744 - Avg. Accuracy: 0.7634 - Val Loss: 2.5085 - Val Accuracy: 0.6579\n",
            "Epoch: 7/15 - Avg. Loss: 1.5565 - Avg. Accuracy: 0.7831 - Val Loss: 2.3301 - Val Accuracy: 0.6822\n",
            "Epoch: 8/15 - Avg. Loss: 1.4327 - Avg. Accuracy: 0.8036 - Val Loss: 2.1572 - Val Accuracy: 0.7072\n",
            "Epoch: 9/15 - Avg. Loss: 1.3137 - Avg. Accuracy: 0.8222 - Val Loss: 1.9996 - Val Accuracy: 0.7341\n",
            "Epoch: 10/15 - Avg. Loss: 1.2253 - Avg. Accuracy: 0.8364 - Val Loss: 1.8356 - Val Accuracy: 0.7599\n",
            "Epoch: 11/15 - Avg. Loss: 1.1067 - Avg. Accuracy: 0.8539 - Val Loss: 1.6945 - Val Accuracy: 0.7784\n",
            "Epoch: 12/15 - Avg. Loss: 1.0253 - Avg. Accuracy: 0.8672 - Val Loss: 1.5470 - Val Accuracy: 0.7988\n",
            "Epoch: 13/15 - Avg. Loss: 0.9175 - Avg. Accuracy: 0.8812 - Val Loss: 1.4164 - Val Accuracy: 0.8194\n",
            "Epoch: 14/15 - Avg. Loss: 0.8455 - Avg. Accuracy: 0.8922 - Val Loss: 1.2972 - Val Accuracy: 0.8369\n",
            "Epoch: 15/15 - Avg. Loss: 0.7656 - Avg. Accuracy: 0.9036 - Val Loss: 1.1808 - Val Accuracy: 0.8514\n",
            "The English sentence is: cheers!\n",
            "The Arabic sentence is: في صحتك.\n",
            "The predicted Arabic sentence is:\n",
            "1/1 [==============================] - 1s 885ms/step\n",
            "في متأخراً            \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}